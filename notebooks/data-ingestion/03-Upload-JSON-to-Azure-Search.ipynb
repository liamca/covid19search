{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all JSON from Blob Container and upload to Azure Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import globals\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "from azure.storage.blob import BlockBlobService\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLocalFile(file_name):\n",
    "    json_content = {}\n",
    "    try:\n",
    "        with open(file_name, 'r') as json_file:\n",
    "            json_content = json.loads(json_file.read())\n",
    "        docID = json_content[\"paper_id\"]\n",
    "        title = json_content[\"metadata\"][\"title\"]\n",
    "\n",
    "        body = {\"documents\": []}\n",
    "        \n",
    "        abstractContent = ''\n",
    "        id_counter = 1\n",
    "        if \"abstract\" in json_content:\n",
    "            for c in json_content[\"abstract\"]:\n",
    "                abstractContent += c[\"text\"] + ' '\n",
    "                body[\"documents\"].append({\n",
    "                      \"language\": \"en\",\n",
    "                      \"id\": str(id_counter),\n",
    "                      \"text\": c[\"text\"]\n",
    "                    })\n",
    "                id_counter += 1\n",
    "\n",
    "        abstractContent = abstractContent.strip()\n",
    "\n",
    "        body = ''\n",
    "        if \"body_text\" in json_content:\n",
    "            for c in json_content[\"body_text\"]:\n",
    "                body += c[\"text\"] + ' '\n",
    "            body = body.strip()\n",
    "\n",
    "        contributors = []\n",
    "        for c in json_content[\"metadata\"][\"authors\"]:\n",
    "            midInitial = ''\n",
    "            for mi in c[\"middle\"]:\n",
    "                midInitial += mi + ' '\n",
    "            if len(((c[\"first\"] + ' ' + midInitial + c[\"last\"]).strip())) > 2:\n",
    "                contributors.append((c[\"first\"] + ' ' + midInitial + c[\"last\"]).strip()) \n",
    "\n",
    "        return {\"@search.action\": \"mergeOrUpload\", \"docID\": docID, \"title\":title, \"abstractContent\": abstractContent, \"body\": body, \"contributors\": contributors}\n",
    "\n",
    "\n",
    "    except Exception as ex:\n",
    "        print (blob_name, \" - Error:\", str(ex))\n",
    "        return \"Error\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(globals.files_dir, 'new_files.pkl'), 'rb') as input:\n",
    "    new_files = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1941 to upload...\n"
     ]
    }
   ],
   "source": [
    "print (str(len(new_files)), 'to upload...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 100 docs...\n",
      "Applying 41 docs...\n"
     ]
    }
   ],
   "source": [
    "documents = {\"value\": []}\n",
    "for json_file in new_files:\n",
    "    # print (json_file[json_file.rindex('/')+1:].replace('.json', '').replace('.xml', ''))\n",
    "    documents[\"value\"].append(processLocalFile(json_file))\n",
    "    if len(documents[\"value\"]) == 100:\n",
    "        print (\"Applying\", str(len(documents[\"value\"])), \"docs...\")\n",
    "        url = globals.endpoint + \"indexes/\" + globals.indexName + \"/docs/index\" + globals.api_version\n",
    "        response  = requests.post(url, headers=globals.headers, json=documents)\n",
    "        documents = {\"value\": []}\n",
    "if len(documents[\"value\"]) > 0:\n",
    "    print (\"Applying\", str(len(documents[\"value\"])), \"docs...\")\n",
    "    url = globals.endpoint + \"indexes/\" + globals.indexName + \"/docs/index\" + globals.api_version\n",
    "    response  = requests.post(url, headers=globals.headers, json=documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_default",
   "language": "python",
   "name": "conda-env-py37_default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
